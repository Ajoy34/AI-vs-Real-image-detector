{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b985ba03",
   "metadata": {},
   "source": [
    "# Novel AI Image Detection System\n",
    "\n",
    "## Novel Approaches Implemented:\n",
    "1. **Physics-Based Lighting Consistency Analysis** - Detects impossible lighting patterns in AI images\n",
    "2. **Semantic Consistency with CLIP** - Verifies semantic coherence using vision-language models\n",
    "3. **Neuromorphic Feature Engineering** - Brain-inspired synchrony and complexity measures\n",
    "4. **Quantum-Inspired Features** - Amplitude and phase-based representation\n",
    "5. **Multi-Scale Wavelet Analysis** - Deep frequency decomposition\n",
    "6. **Adversarial Robustness Testing** - Foolbox integration for model hardening\n",
    "7. **Ensemble with Specialized Classifiers** - CatBoost, XGBoost, MLP, SVM\n",
    "\n",
    "## Installation Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944879f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_wavelets foolbox ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42c596",
   "metadata": {},
   "source": [
    "## Part 1: Feature Extraction Pipeline\n",
    "This cell contains all feature extraction functions including novel physics-based and semantic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.amp import autocast\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import kornia\n",
    "from pytorch_wavelets import DWTForward\n",
    "import gc\n",
    "import warnings\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import least_squares\n",
    "import numpy.linalg as la\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "    CLIP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"[WARN] CLIP not available. Install with: pip install git+https://github.com/openai/CLIP.git\")\n",
    "    CLIP_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Global configuration\n",
    "USE_DEEP = True\n",
    "BATCH_SIZE = 576\n",
    "DEEP_FEATURE_DIM = 128\n",
    "\n",
    "# Memory management decorator\n",
    "def memory_cleanup(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Deep feature extractor\n",
    "if USE_DEEP:\n",
    "    print(\"[INFO] Loading MobileNetV3 (feature extractor)...\")\n",
    "    mobilenet = models.mobilenet_v3_small(weights='IMAGENET1K_V1').to(DEVICE)\n",
    "    mobilenet.classifier = nn.Identity()\n",
    "    mobilenet.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "@memory_cleanup\n",
    "def extract_deep_features(img_rgb, pca=None):\n",
    "    try:\n",
    "        img_t = transform(img_rgb).unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad(), autocast('cuda' if DEVICE == 'cuda' else 'cpu'):\n",
    "            feat = mobilenet(img_t)\n",
    "        feat = feat.cpu().numpy().flatten()\n",
    "        \n",
    "        if pca is not None and hasattr(pca, 'components_'):\n",
    "            feat = pca.transform(feat.reshape(1, -1)).flatten()\n",
    "            if len(feat) > DEEP_FEATURE_DIM:\n",
    "                feat = feat[:DEEP_FEATURE_DIM]\n",
    "            elif len(feat) < DEEP_FEATURE_DIM:\n",
    "                feat = np.pad(feat, (0, DEEP_FEATURE_DIM - len(feat)))\n",
    "        else:\n",
    "            feat = feat[:DEEP_FEATURE_DIM] if len(feat) > DEEP_FEATURE_DIM else np.pad(feat, (0, DEEP_FEATURE_DIM - len(feat)))\n",
    "        \n",
    "        return feat\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Deep feature extraction failed: {e}\")\n",
    "        return np.zeros(DEEP_FEATURE_DIM)\n",
    "\n",
    "# GPU-accelerated feature extractors\n",
    "@memory_cleanup\n",
    "def compute_sobel_features(img_tensor):\n",
    "    try:\n",
    "        if img_tensor.shape[-2:] < (16, 16):\n",
    "            return [0.0] * 3, [\"sobel_mean\", \"sobel_std\", \"sobel_edge_density\"]\n",
    "        gray = kornia.color.rgb_to_grayscale(img_tensor.unsqueeze(0))\n",
    "        with autocast('cuda' if DEVICE == 'cuda' else 'cpu'):\n",
    "            sobel = kornia.filters.sobel(gray)\n",
    "            mag = torch.norm(sobel, dim=1)\n",
    "        feats = [mag.mean().item(), mag.std().item(), (mag > 0.05).float().mean().item()]\n",
    "        return feats, [\"sobel_mean\", \"sobel_std\", \"sobel_edge_density\"]\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Sobel computation failed: {e}\")\n",
    "        return [0.0] * 3, [\"sobel_mean\", \"sobel_std\", \"sobel_edge_density\"]\n",
    "\n",
    "@memory_cleanup\n",
    "def compute_fft_band_energies(img_tensor):\n",
    "    try:\n",
    "        if img_tensor.shape[-2:] < (16, 16):\n",
    "            return [0.0] * 5, [\"fft_mean\", \"fft_std\", \"fft_low\", \"fft_mid\", \"fft_high\"]\n",
    "        \n",
    "        gray = kornia.color.rgb_to_grayscale(img_tensor.unsqueeze(0))\n",
    "        gray_2d = gray.squeeze(0).squeeze(0)\n",
    "        \n",
    "        with autocast('cuda' if DEVICE == 'cuda' else 'cpu'):\n",
    "            fft = torch.fft.fft2(gray_2d)\n",
    "            fft_shift = torch.fft.fftshift(fft)\n",
    "            mag = torch.log(torch.abs(fft_shift) + 1e-8)\n",
    "        \n",
    "        H, W = mag.shape\n",
    "        if H == 0 or W == 0:\n",
    "            return [0.0] * 5, [\"fft_mean\", \"fft_std\", \"fft_low\", \"fft_mid\", \"fft_high\"]\n",
    "            \n",
    "        cy, cx = H // 2, W // 2\n",
    "        maxr = min(H, W) // 2\n",
    "        \n",
    "        if maxr <= 0:\n",
    "            return [0.0] * 5, [\"fft_mean\", \"fft_std\", \"fft_low\", \"fft_mid\", \"fft_high\"]\n",
    "            \n",
    "        r1, r2 = max(1, maxr // 4), max(1, maxr // 2)\n",
    "        \n",
    "        Y, X = torch.meshgrid(torch.arange(H, device=mag.device), \n",
    "                             torch.arange(W, device=mag.device), indexing='ij')\n",
    "        dist2 = (X - cx) ** 2 + (Y - cy) ** 2\n",
    "        \n",
    "        low_mask = dist2 <= r1 ** 2\n",
    "        mid_mask = (dist2 > r1 ** 2) & (dist2 <= r2 ** 2)\n",
    "        high_mask = dist2 > r2 ** 2\n",
    "        \n",
    "        low = mag[low_mask].mean().item() if low_mask.any() else 0.0\n",
    "        mid = mag[mid_mask].mean().item() if mid_mask.any() else 0.0\n",
    "        high = mag[high_mask].mean().item() if high_mask.any() else 0.0\n",
    "        \n",
    "        feats = [mag.mean().item(), mag.std().item(), low, mid, high]\n",
    "        return feats, [\"fft_mean\", \"fft_std\", \"fft_low\", \"fft_mid\", \"fft_high\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] FFT computation failed: {e}\")\n",
    "        return [0.0] * 5, [\"fft_mean\", \"fft_std\", \"fft_low\", \"fft_mid\", \"fft_high\"]\n",
    "\n",
    "@memory_cleanup\n",
    "def compute_lbp_torch(img_tensor, bins=16):\n",
    "    try:\n",
    "        if img_tensor.shape[-2:] < (16, 16):\n",
    "            return [0.0] * bins, [f\"lbp_bin{i}\" for i in range(bins)]\n",
    "        \n",
    "        gray = kornia.color.rgb_to_grayscale(img_tensor.unsqueeze(0))\n",
    "        gray_2d = gray.squeeze(0).squeeze(0)\n",
    "        \n",
    "        pad = F.pad(gray_2d.unsqueeze(0).unsqueeze(0), (1, 1, 1, 1), mode='constant', value=0)\n",
    "        pad = pad.squeeze(0).squeeze(0)\n",
    "        \n",
    "        H, W = gray_2d.shape\n",
    "        lbp = torch.zeros(H, W, device=gray_2d.device)\n",
    "        \n",
    "        offsets = [(-1, -1), (-1, 0), (-1, 1),\n",
    "                  (0, 1), (1, 1), (1, 0), \n",
    "                  (1, -1), (0, -1)]\n",
    "        \n",
    "        center = gray_2d[1:H-1, 1:W-1]\n",
    "        \n",
    "        for i, (dy, dx) in enumerate(offsets):\n",
    "            neighbor = pad[1+dy:H-1+dy, 1+dx:W-1+dx]\n",
    "            lbp[1:H-1, 1:W-1] += ((neighbor >= center) * (2 ** i)).float()\n",
    "        \n",
    "        lbp_flat = lbp.flatten().cpu()\n",
    "        hist = torch.histc(lbp_flat, bins=bins, min=0, max=255)\n",
    "        hist = hist / (hist.sum() + 1e-8)\n",
    "        \n",
    "        return hist.numpy().tolist(), [f\"lbp_bin{i}\" for i in range(bins)]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] LBP computation failed: {e}\")\n",
    "        return [0.0] * bins, [f\"lbp_bin{i}\" for i in range(bins)]\n",
    "\n",
    "@memory_cleanup\n",
    "def compute_color_stats(img_tensor):\n",
    "    \"\"\"FIXED: Removed emoji character\"\"\"\n",
    "    try:\n",
    "        if img_tensor.shape[-2:] < (16, 16):\n",
    "            return [0.0] * 18, [f\"{prefix}{i}_{stat}\" for prefix in [\"rgb\", \"hsv\", \"lab\"] for i in range(3) for stat in [\"mean\", \"std\"]]\n",
    "        hsv = kornia.color.rgb_to_hsv(img_tensor.unsqueeze(0))\n",
    "        lab = kornia.color.rgb_to_lab(img_tensor.unsqueeze(0))\n",
    "        feats, names = [], []\n",
    "        for space, prefix in zip([img_tensor.unsqueeze(0), hsv, lab], [\"rgb\", \"hsv\", \"lab\"]):\n",
    "            for i in range(3):\n",
    "                ch = space[:, i, :, :]  # FIXED: Was ch = space[:, i, :, ðŸ™‚\n",
    "                feats.append(ch.mean().item())\n",
    "                names.append(f\"{prefix}{i}_mean\")\n",
    "                feats.append(ch.std().item())\n",
    "                names.append(f\"{prefix}{i}_std\")\n",
    "        return feats, names\n",
    "    except Exception as e:\n",
    "        print(f\"[DEBUG] Color stats computation failed: {e}\")\n",
    "        return [0.0] * 18, [f\"{prefix}{i}_{stat}\" for prefix in [\"rgb\", \"hsv\", \"lab\"] for i in range(3) for stat in [\"mean\", \"std\"]]\n",
    "\n",
    "# Continue with more feature extractors...\n",
    "print(\"[INFO] Feature extraction functions loaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
